{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IEEE Floating Point Arithmetic\n\n\nReference: [Overton](https://cs.nyu.edu/~overton/book/) Chapter 4.\n\nIn this lecture, we introduce the\n[IEEE Standard for Floating-Point Arithmetic](https://en.wikipedia.org/wiki/IEEE_754).\nThere are many  possible ways of representing real numbers on a computer, as well as \nthe precise behaviour of operations such as addition, multiplication, etc.\nBefore the 1980s each processor had potentially a different representation for \nreal numbers, as well as different behaviour for operations.  \nIEEE introduced in 1985 was a means to standardise this across\nprocessors so that algorithms would produce consistent and reliable results.\n\nThis chapter may seem very low level for a mathematics course but there are\ntwo important reasons to understand the behaviour of floating point numbers:\n1. Floating point arithmetic is very precisely defined, and can even be used\nin rigorous computations as we shall see in the problem sheets.\n2. Failure to understand floating point arithmetic can cause many errors\nin practice, with the extreme example being the [explosion of the Ariane 5 rocket](https://youtu.be/N6PWATvLQCY?t=86).\n\n\nBefore we begin, we load two external packages. SetRounding.jl allows us \nto set the rounding mode of floating point arithmetic. ColorBitstring.jl\n  implements a function `printbits`\nwhich print the bits of floating point numbers in colour. \nEach colour corresponds to a different part of the representation:\nthe <span style=\"color:red\">sign bit</span>, the <span style=\"color:green\">exponent bits</span>, \nand the <span style=\"color:blue\">significand bits</span> which we shall learn about shortly."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SetRounding, ColorBitstring\nprintbits(1.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter we discuss the following:\n\n1. Integers:  There are multiple ways of representing integers on a computer. Here we discuss the \nthe different types of integers and their representation as bits, and how arithmetic operations behave \nlike modular arithmetic.\n2. Binary representation of real numbers: Any real number can be represented by an infinite sequence of bits. \n2. Floating point numbers: we discuss how real numbers are stored on a computer with a finite number of bits.\n3. Arithmetic: we discuss how arithmetic operations in floating point are exact up to rounding, and how the\nrounding mode can be set. This allows for precise error bounds in computations.\n4. High-precision floating point numbers: we discuss how the precision of floating point arithmetic can be increased arbitrary\nusing `BigFloat`.\n\n## 1. Integers\n\nJulia uses a math-like syntax for manipulating integers:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "1 + 1 # Addition"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "2 * 3 # Multiplication"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 5; # semicolon is optional but supresses output if used in the last line\nx^2 # Powers"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Julia everything has a type. This is similar in spirit to\na class in Python, but much more lightweight.\nAn integer defaults to a type `Int`,\nwhich is either 32-bit (`Int32`) or 64-bit (`Int64`) depending\non the processor of the machine. There are also 8-bit (`Int8`), 16-bit (`Int16`), \nand 128-bit (`Int128`) integer types, which\nwe can construct by converting an `Int`, e.g. `Int8(3)`.\n\nThese are all \"primitive types\", instances of the type are stored in memory as\na fixed length sequence of bits.\nWe can find the type of a variable as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "typeof(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a primitive type we can see the bits using the function `bitstring`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bitstring(Int8(1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative numbers may be surprising:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bitstring(-Int8(1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This format is known as [2's complemement](https://epubs.siam.org/doi/abs/10.1137/1.9780898718072.ch3).\nIt may be counter-intuitive (did you expect `10000001`?)\nbut has the nice benefit that addition behaves the same for\npositive integers and negative integers. \n\n`Int` follows [modular arithmetic](https://en.wikipedia.org/wiki/Modular_arithmetic),\nthat is, it is equivalent to ${\\mathbb Z}_{2^p}$, \nthe ring of integers modulo `2^p` (where `p` is `8`, `16`, `32` or `64`).\nThus for $p = 8$ we are interpreting\n$2^4 \\mod 2^8$ through $(2^8-1) \\mod 2^8$ as negative numbers but\nthey are not treated specially. That is, the bits for a negative integer $-y$ are the same\nas the binary representation of $2^p - y$. In particular,  an integer is negative if and only if \nthe first bit (corresponding to the $2^{p-1}$ place) is `1`.\n\n**Example (converting bits to integers)** \nWhat 8-bit number has the bits `01001001`? Adding the corresponding decimal places we get:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "2^0 + 2^3 + 2^6"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What 8-bit number has the bits `11001001`? Because the first bit is `1` we know it's a negative \nnumber, hence we need to sum the bits but then subtract `2^p`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "2^0 + 2^3 + 2^6 + 2^7 - 2^8"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the results using `bitstring`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bitstring(Int8(73)), bitstring(-Int8(55))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example (addition)**\nConsider `(-1) + 1`. They behave like modular arithmetic so we have:\n$$\n(-1 \\mod 2^p) + (1 \\mod 2^p) = (2^p-1 \\mod 2^p) + (1 \\mod 2^p) = 2^p \\mod 2^p = 0 \\mod 2^p\n$$\nIn other words, the bits of this addition are all 0.\n\n**Example (multiplication)**\nConsider `(-2) * 2`. We have:\n$$\n(-2 \\mod 2^p) * (2 \\mod 2^p) = (2^p-2 \\mod 2^p) * (2 \\mod 2^p) = (2^{p+1}-4 \\mod 2^p) = -4 \\mod 2^p\n$$\nIn other words, the bits are the same as the binary representation of $2^{p+1} - 4$, which\nwe interpret as $-4$.\n\n\n\n\nWe can find the largest and smallest instances of a type using `typemax` and `typemin`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "typemax(Int8)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the largest positive number, that is, the sign bit is zero and all other bits are 1:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bitstring(typemax(Int8))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, the smallest integer is:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "typemin(Int8)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has sign bit `1` but all other bits are zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bitstring(typemin(Int8))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since `Int64` behaves like modulo arithmetic we do not \"overflow\" but rather wrap around:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "typemax(Int8) + Int8(1) # returns typemin(Int8)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other primitive integer types:  `UInt8`, `UInt16`, `UInt32`, and `UInt64` are unsigned integers, \ne.g., we do not interpret\nthe number as negative if the first bit is `1`. A non-primitive type is `BigInt` which allows arbitrary length\nintegers, which we can create using `big`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = typemax(Int64) + big(1) # Too big to be an `Int64`"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note in this case it automatically promotes an `Int64` to a `BigInt`.\nWe can create very large numbers using `BigInt`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x^100"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the number of bits is not fixed, the larger the number the more bits are needed \nto represent it, so it is possible to run out of memory if a number is\nastronomically large: go ahead and try `x^x` (at your own risk).\n\nIn addition to `+`, `-`, and `*` we have integer division `÷`, which rounds down:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "5 ÷ 2 # equivalent to div(5,2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard division `/` (or `\\` for division on the right) creates a floating point number, which will be discussed in\nthe next chapter:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "5 / 2 # alternatively 2 \\ 5"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create rational numbers using `//`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "(1//2) + (3//4)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remark** Rational arithmetic often leads to overflow so it\nis often best to combine `big` with rationals:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "102324//132413023 + 23434545//4243061 + 23434545//42430534435\nbig(102324)//132413023 + 23434545//4243061 + 23434545//42430534435"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.  Binary representation of real numbers\n\nBefore discussing how to represent real numbers with a finite number\nof bits, we consider what one could do with an infinite number of bits,\nthat is, binary expansions.\n\nWe use the notation:\n$$\n(b_0.b_1b_2b_3\\ldots)_2 = b_0 + {b_1 \\over 2} + {b_2 \\over 2^2} + {b_3 \\over 2^3} + \\cdots\n$$\nwhere $b_k$ are either 0 or 1. Every number $0 \\leq x < 1$ has a binary representation in this form.\nFirst we show some examples of verifying a numbers binary representation:\n\n**Example**\nConsider the number `1/3`.  In decimal we all know:\n$$\n1/3 = 0.3333\\ldots =  \\sum_{k=1}^\\infty {3 \\over 10^k}\n$$\nWe will see that in binary\n$$\n1/3 = (0.010101\\ldots)_2 = \\sum_{k=1}^\\infty {1 \\over 2^{2k}}\n$$\nBoth results can be proven using the geometric series:\n$$\n\\sum_{k=0}^\\infty z^k = {1 \\over 1 - z}\n$$\nprovided $|z| < 1$. That is, with $z = 1/10$ we see the decimal case:\n$$\n3\\sum_{k=1}^\\infty {1 \\over 10^k} = {3 \\over 1 - {1 \\over 10}} - 3 = {1 \\over 3}\n$$\nA similar argument works for binary with $z = {1 \\over 4}$:\n$$\n\\sum_{k=1}^\\infty {1 \\over 4^k} = {1 \\over 1 - 1/4} - 1 = {1 \\over 3}\n$$\n\nThe extension to numbers outside the range $0 \\leq x < 1$ is clear, e.g.\n$$\n(-101.010101\\ldots)_2 = -(5 + 1/3)\n$$\nBut for floating point numbers we handle the integer part differently so\nwe will not use this.\n\n\n\n\n## 2. Floating point numbers\n\nFloating point numbers are how a computer typically approximates\na real number. \n\n**Definition** The _floating point numbers_ are\n$$\nF_{S,Q,P} := F^{\\rm normal}_{S,Q,P} \\cup F^{\\rm sub-normal}_{S,Q,P} \\cup F^{\\rm special}.\n$$\nThe _normal floating point numbers_\n$F^{\\rm normal}_{S,Q,P} \\subset {\\mathbb R}$ are defined by\n$$\nF^{\\rm normal}_{S,Q,P} = \\{\\pm 2^{q-S} \\times (1.b_1b_2b_3\\ldots b_P)_2 : 1 \\leq q < 2^Q-1 \\}.\n$$\nThe _sub-normal numbers_ $F^{\\rm sub-normal}_{S,Q,P} \\subset {\\mathbb R}$ are defined as\n$$\nF^{\\rm sub-normal}_{S,Q,P} = \\{\\pm 2^{1-S} \\times (0.b_1b_2b_3\\ldots b_P)_2\\}.\n$$\nThe _special numbers_ $F^{\\rm special} \\not\\subset {\\mathbb R}$ are defined later.\n\nNote this set of real numbers has no nice algebraic structure: it is not closed under addition, subtraction, etc.\nand the normal floating point numbers does not include $0$. We will therefore need to define approximate versions of algebraic operations later.\n\nFloating point numbers are stored in $1 + Q + P$ total number of bits, in the format\n$$\nsq_1\\ldots q_Q b_1 \\ldots b_P\n$$\nThe first bit ($s$) is the <span style=\"color:red\">sign bit</span>: 0 means positive and 1 means\nnegative. The bits $q_1\\ldots q_Q$ are the <span style=\"color:green\">exponent bits</span>:\nthey are the binary digits of the unsigned integer $q$: \n$$\nq = (q_1\\ldots q_Q)_2.\n$$\nFinally, the bits $b_1\\ldots b_P$ are the <span style=\"color:blue\">significand bits</span>.\nIf $1 \\leq q < 2^Q-1$ then the bits represent the normal number\n$$\nx = \\pm 2^{q-S} \\times (1.b_1b_2b_3\\ldots b_P)_2.\n$$\nIf $q = 0$ (i.e. all bits are 0) then the bits represent the sub-normal number\n$$\nx = \\pm 2^{1-S} \\times (0.b_1b_2b_3\\ldots b_P)_2.\n$$\nIf $q = 2^Q-1$  (i.e. all bits are 1) then the bits represent a special number, discussed\nlater.\n\n\n### IEEE floating point numbers\n\n**Definition** IEEE has 3 standard floating point numbers: 16-bit (half precision), 32-bit (single precision) and\n64-bit (double precision) defined by:\n$$\n\\begin{align*}\nF_{16} &:= F_{15,5,10} \\\\\nF_{32} &:= F_{127,8,23} \\\\\nF_{64} &:= F_{1023,11,52}\n\\end{align*}\n$$\n\nIn Julia these correspond to 3 different floating point types:\n\n1.  `Float64` is a type representing $F_{64}$.\nWe can create a `Float64` by including a \ndecimal point when writing the number: \n`1.0` is a `Float64`. `Float64` is the default format for \nscientific computing (on the _Floating Point Unit_, FPU).  \n2. `Float32` is a type representing $F_{32}$.  We can create a `Float32` by including a \n`f0` when writing the number: \n`1f0` is a `Float32`. is generally the default format for graphics (on the _Graphics Processing Unit_, GPU), \nas the difference between 32 bits and 64 bits is indistinguishable to the eye in visualisation,\nand more data can be fit into a GPU's limitted memory.\n3.  `Float16` is a type representing $F_{16}$.\nIt is important in machine learning where one wants to maximise the amount of data\nand high accuracy is not necessarily helpful. \n\n\n**Example** How is the number $1/3$ stored in `Float32`?\nRecall that\n$$\n1/3 = (0.010101\\ldots)_2 = 2^{-2} (1.0101\\ldots)_2 = 2^{125-127} (1.0101\\ldots)_2\n$$\nSince\n$$\n125 = (1111101)_2\n$$\nFor the significand we round the last bit to the nearest (this is explained in detail in\nthe section on rounding), so we have\n$$\n1.010101010101010101010101\\ldots \\approx 1.01010101010101010101011 \n$$\nThus the `Float32` bits for $1/3$ are:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(1f0/3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sub-normal numbers, the simplest example is zero, which has $q=0$ and all significand bits zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(0.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike integers, we also have a negative zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(-0.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is treated as identical to `0.0`, except for with regards to special numbers.\n\n\nIf we divide the smallest normal number by two, we get a subnormal number:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(mn/2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you explain the bits?\n\n### Important normal numbers\n\nWhen dealing with normal numbers there are some important constants that we will use\nto bound errors.\n\n**Definition** _Machine epsilon_ is denoted\n$$\n\\epsilon_{{\\rm m},P} := 2^{-P}.\n$$\nWhen $P$ is implied by context we use the notation $\\epsilon_{\\rm m}$.\n\nThe _smallest positive normal number_ is $q = 1$ and $b_k$ all zero:\n$$\n\\min |F_{S,P,Q}^{\\rm normal}| = 2^{1-S}.\n$$ \nThe _largest (positive) normal number_ is \n$$\n\\max F_{S,P,Q}^{\\rm normal} = 2^{2^Q-2-S} (1.11\\ldots1)_2 = 2^{2^Q-2-S} (2-\\epsilon_{\\rm m})\n$$\nWe confirm the simple bit representations:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "S,P,Q = 127,23,8 # Float32\nεₘ = 2.0^(-P)\nprintlnbits(Float32(2.0^(1-S))) # smallest positive Float32\nprintlnbits(Float32(2.0^(2^Q-2-S) * (2-εₘ))) # largest Float32"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a given floating point type, we can find these constants using the following functions:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "eps(Float32),floatmin(Float32),floatmax(Float32)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special numbers\n\nThe special numbers extend the real line by adding $\\pm \\infty$ but also a notion of \"not-a-number\".\n\n**Definition**\nDenote ${\\rm NaN} := \\{\\}$ and define\n$$\nF^{\\rm special} := \\{\\infty, -\\infty, {\\rm NaN}\\}\n$$\n\nWhenever the bits of $q$ of a floating point number are all 1 then they represent an element of $F^{\\rm special}$.\nIf all $b_k=0$, then the number represents either $\\pm\\infty$, called `Inf` and `-Inf`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printlnbits(Inf)\nprintbits(-Inf)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "All other special floating point numbers represent `NaN`: `NaN` is stored with $q=(11111111111)_2$ and at least one of the $b_k =1$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(NaN)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are needed for undefined algebraic operations such as:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "0/0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example** What happens if we change some other $b_k$ to be nonzero?\nWe can create bits as a string and see:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "i=parse(UInt64, \"1111111111110000000000000000000010000001000000000010000000000000\"; base=2)\nreinterpret(Float64,i)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, there are more than one `NaN`s on a computer.  Can you figure out how many there are?\n\n\n## 3. Arithmetic\n\n\nArithmetic operations are done exactly _up to rounding_.\nThere are three basic rounding strategies: round up/down/towards zero/nearest.\nMathematically we introduce the function ${\\rm round}$:\n\n**Definition** ${\\rm round}^{\\rm up}_{S,Q,P} : \\mathbb R \\rightarrow F_{S,Q,P}$ denotes \nthe function that rounds a real number up to the nearest floating point number that is greater or equal.\n${\\rm round}^{\\rm down}_{S,Q,P} : \\mathbb R \\rightarrow F_{S,Q,P}$ denotes \nthe function that rounds a real number down to the nearest floating point number that is greater or equal.\n${\\rm round}^{\\rm nearest}_{S,Q,P} : \\mathbb R \\rightarrow F_{S,Q,P}$ denotes \nthe function that rounds a real number to the nearest floating point number. In case of a tie, the\nit returns the floating point number whose least significant bit is equal to zero.\nWe use the notation ${\\rm round}$ when $S,Q,P$ and the rounding mode are implied by context,\nwith ${\\rm round}^{\\rm nearest}$ being the default rounding mode.\n\n\n\nIn Julia, the rounding mode is specified by tags `RoundUp`, `RoundDown`, `RoundToZero`, and\n`RoundNearest`. (There are also more exotic rounding strategies `RoundNearestTiesAway` and\n`RoundNearestTiesUp` that we won't use.)\n Note that these rounding modes are part\nof the FPU instruction set so will be (roughly) equally fast as the default, `RoundNearest`.\n\nLet's try rounding a `Float64` to a `Float32`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(1/3)  # 64 bits\nprintbits(Float32(1/3))  # round to nearest 32-bit"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default rounding mode can be changed:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "printbits(Float32(1/3,RoundDown) )"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or alternatively we can change the rounding mode for a chunk of code\nusing `setrounding`. The following changes `/` to round down:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "setrounding(Float32, RoundDown) do\n    1f0/3\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In IEEE arithmetic, the arithmetic operations `+`, `-`, `*`, `/` are defined by the property that they are exact up to \nrounding.  Mathematically we denote these operations as follows:\n$$\nx\\oplus y &:= {\\rm round}(x+y) \\\\\nx\\ominus y &:= {\\rm round}(x - y) \\\\\nx\\otimes y &:= {\\rm round}(x * y) \\\\\nx\\odiv y &:= {\\rm round}(x / y)\n$$\nNote also that  `^` and `sqrt` are similarly exact up to rounding.\n\n**WARNING** These operations are not associative! E.g. $(x \\oplus y) \\oplus z$ is not necessarily equal to $x \\oplus (y \\oplus z)$. \nCommutativity is preserved at least.\n\n\n**Example** `1.1+0.1` gives a different result than `1.2`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x=1.1\ny=0.1\nx + y - 1.2 # Not Zero?!?"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because ${\\rm round}(1.1) \\neq 1+1/10$, but rather:\n$$\n{\\rm round}(1.1) = 1 + 2^{-4}+2^{-5} + 2^{-8}+2^{-9}+\\cdots + 2^{-48}+2^{-49} + 2^{-51}= {2476979795053773 \\over 2251799813685248} = 1.1 +2^{-51} - 2^{-52} - 2^{-53} - 2^{-56} - 2^{-57} - \\cdots\n$$\n\n\nWe can bound basic arithmetic operations in terms of:\n\n**Definition** The _normalized range_ ${\\cal N}_{S,Q,P} \\subset {\\mathbb R}$\nis the subset of real numbers that lies\nbetween the smallest and largest normal floating point number:\n$$\n{\\cal N}_{S,Q,P} := \\{x : 2^{1-S} \\leq |x| \\leq \\}\n$$\nWhen $S,Q,P$ are implied by context we use the notation ${\\rm N}$.\n\nThis corresponds to the _relative error_ introduced\nby changing the last bit of a normal floating point number. \n\n**Proposition** Let $m$ and $M$ denote the smallest and largest positive element of\n$F_{S,Q,P}^{\\rm normal}$. If $m \\leq x \\leq M$ then for any rounding mode we have:\n$$\n|{\\rm round}(x) - x|  \\leq |x| (1 + \\epsilon_{\\rm m})\n$$\n\n**Lemma**\nFor any rounding mode we have:\n$$\n\\begin{align*}\n|x \\oplus y - (x + y)| \\leq |x+y| \\epsilon_{\\rm m} \\\\\n|x \\ominus y - (x - y)| \\leq |x-y| \\epsilon_{\\rm m} \\\\\n|x \\oplus y - (x + y)| \\leq |x+y| \\epsilon_{\\rm m} \\\\\n|x \\oplus y - (x + y)| \\leq |x+y| \\epsilon_{\\rm m}.\n\\end{align*}\n$$\n\n\n### Arithmetic and special numbers\n\nArithmetic works differently on `Inf` and `NaN`. In particular we have:\n$$\n\\begin{align*}\nx \\odiv 0 &:= \\begin{cases}\n    (\\sign x) \\infty & x \\neq 0 \\\\\n    {\\rm NaN} & {\\rm otherwise}\\\\\n\\end{\\align*}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Inf*0      # NaN\nInf+5      # Inf\n(-1)*Inf   # -Inf\n1/Inf      # 0\n1/(-Inf)   # -0\nInf-Inf    # NaN\nInf == Inf   # true\nInf == -Inf  # false\n\nNaN*0      # NaN\nNaN+5      # NaN\n1/NaN      # NaN\nNaN == NaN    # false\nNaN != NaN    #true"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special functions\n\nOther special functions like `cos`, `sin`, `exp`, etc. are _not_ part of the IEEE standard.\nInstead, they are implemented by composing the basic arithmetic operations, which accumulate\nerrors. Fortunately they are all designed to have relative accuracy, that is, there exists\nreasonably small $c > 0$ such that, for `s = sin(x)` (that is, the Julia implementation of $sin(x)$) satisfies\n$$\n|s - \\sin x| \\leq |sin(x)| c\\epsilon_{\\rm m}\n$$\nNote these special functions are written in (advanced) Julia code, for example, \n[sin](https://github.com/JuliaLang/julia/blob/d08b05df6f01cf4ec6e4c28ad94cedda76cc62e8/base/special/trig.jl#L76).\n\n\n**WARNING** This is an extremely misleading statement for large `x`. Consider\nthe following demonstration of the statement:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ε = eps() # machine epsilon, 2^(-52)\nx = 2*10.0^100\nabs(sin(x) - sin(big(x)))  ≤  abs(sin(big(x))) * ε"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we instead compute `10^100` using `BigFloat` we get a completely different\nanswer that even has the wrong sign!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x̃ = 2*big(10.0)^100\nsin(x), sin(x̃)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because we commit an error on the order of roughly $2 * 10^100 * \\epsilon_{\\machine} \\approx 4.44 * 10^(84)$\nwhen we round $2*10^100$ to the nearest float. \n\n## 4. High-precision floating point numbers (*advanced*)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.1"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
