# Differentiation

We now get to our first computational problem: given a function, how can we approximate its derivative at a
point?
We consider three possible scenarios:

1. The function is a "black-box": we can only evaluate it on, say, `Float64` inputs. This is the situation
if we have a function that relies on C code.
2. The function is a piece of code, and we can evaluate it on arbitrary types, including
ones we create. This is the case if we have a function defined in Julia that does not call any
C libraries.
3. The function is built by composing different basic "kernels" with known differentiability properties.
We won't consider this situation in this module, though it is the model used by Python machine learning toolbox's
like [PyTorch](https://pytorch.org) and [TensorFlow](http://tensorflow.org).

We discuss the following techniques:

1. Finite-differences: this uses the definition of a derivative that one learns in calculus to approximate its value.
2. Dual numbers and forward-mode automatic differentiation: we define a special type that when applied to a function
computes its derivative. This unsafe_store! _dual numbers_, which is analoguous to complex numbers.
3. Adjoints and reverse-mode automatic differentiation: this is similar to applying the "chain rule" by building up
a tape of operations. It's outside the scope of this module but is important for machine learning.
4. Interpolation and differentiation: this will be discussed later in the module.


## 1. Finite-differences

The definition 
$$
f'(x) = \lim_{h \rightarrow 0} {f(x+h) - f(x) \over h}
$$
tells us that
$$
f'(x) \approx {f(x+h) - f(x) \over h}
$$
provided that $h$ is sufficiently small. 

It's important to note that approximation uses only the _black-box_
notion of a function but to obtain bounds we need more.

If we know a bound on $f''(x)$ then Taylor's theorem tells us a precise bound:

**Proposition**
The error in approximating the derivative using finite differences is
$$
\left|f'(x) - {f(x+h) - f(x) \over h}\right| \leq {\sup_{x \leq t \leq x+h} f''(t) \over 2 h}
$$

**Proof**
Follows immediately from Taylor's theorem:
$$
f(x+h) = f(x) + f'(x) h + {f''(\chi) \over 2} h^2
$$
for some $x < \chi < x+h$.

◼️


Note this is assuming _real arithmetic_, the answer is drastically
different with _floating point arithmetic_.

There are also alternative versions of finite differences. Leftside finite-differences:
$$
f'(x) ≈ {f(x) - f(x-h) \over h}
$$
and central differences:
$$
f'(x) ≈ {f(x + h/2) - f(x - h/2) \over h}
$$



### Does finite-differences work with floating point arithmetic?


Let's try differentiating two simple polynomials $f(x) = x + x^2$ and $g(x) = 1 + x + x^2$:
```julia
f = x -> x + x^2     # we treat f and g as black-boxs
g = x -> 1 + x + x^2
h = 0.000001
(f(h)-f(0))/h, (g(h)-g(0))/h
```
Both seem to roughly approximate the true derivatives (`1`).
We can do a plot to see how fast the error goes down as we let $h$ become small. 
```julia
using Plots
h = 10.0 .^ (0:-1:-16)  # [1,0.1,0.01,…,1E-7]
plot(abs.((f.(h) .- f(0)) ./ h .- 1); yscale=:log10, title="convergence of derivatives", label="f", legend=:bottomleft)
plot!(abs.((g.(h) .- g(0)) ./ h .- 1); yscale=:log10, label = "g")
```
In the case of $f$ it is a success: we approximate the true derivative to roughly machine precision,
which is the best one can really hope for given round-off error (i.e., as the algebraic manipulation gets
the last bit wrong) by letting $h$ be sufficiently small.
But for $g$ it is a huge failure: the approximation starts to converge, but then diverges exponentially fast!

We can bound the error using the bounds on floating point arithmetic. For simplicity we ignore the
$x^2$ term: when $h$ is small, $h^2$ gets lost in the rounding. Then we have
$$
\left|((1 \oplus h) \ominus h) - 1 \odiv h -1\right| \leq ((1+h)*(1+\epsilon_{\rm machine}) - 1)*(1+\epsilon_{\rm machine}) = h + h \epsilon_{\rm machine} + O(h^2)
$$
Thus we see that the bound grows like $\epsilon_{\rm machine}/h$. However, a bad upper bound is not the same
as a proof that something grows, and establishing a lower bound on the error is more challenging. In fact,
it depends on the precise bits of $h$. 

**Theorem** Suppose $f : \mathbb R \rightarrow \mathbb R$ and $f^{\rm fp}$ is its implementation in floating
point arithmetic. Assume for $x \in F^{\rm normal}$ that
$$
|f^{\rm fp}(x) - f(x)| \leq |f(x)| (1 + c \epsilon_{\rm m}).
$$
Then 
$$
\left|f'(x) - {(f^{\rm fp}(x+h) \ominus f^{\rm fp}(x)) \oslash h} \right| \leq |f'(x)|
$$

Note a bad upper bound is not the same as a proof that an approximation is bad. And indeed, for our example
taking a step-size of $h = 2^q$ delivers much better results, and in fact gets the _exact_ answer for sufficiently
small `h`, down to the last bit!
```julia
h = 2.0 ^ (-30)
abs((g(h) - g(0)) / h - 1)
```
This is a quirk of this particular function and other functions will not be so lucky. But that said,
establishing a precise condition on when finite-difference is _guaranteed_ to be inaccurate is hard.


**Remark** While finite differences is of questionable utility for computing derivatives, it is extremely effective
in building methods for solving differential equations, as we shall see later.

## 2. Dual numbers (Forward-mode automatic differentiation)

Automatic differentiation consists of applying functions to special types that determine the derivatives.
Here we do so via _dual numbers_.

**Definition (Dual numbers)** Dual numbers ${\mathbb D}$ are a commutative ring over the reals 
generated by $1$ and $\epsilon$ such that $\epsilon^2 = 0$.
Dual numbers are typically written as $a + b \epsilon$ where $a$ and $b$ are real.

This is very much analoguous to complex numbers, which are a field generated by $1$ and ${\rm i}$ such that
${\rm i}^2 = -1$. Compare multiplication of each number type:
$$
\begin{align*}
(a + b {\rm i}) (c + d {\rm i}) &= ac + (bc + ad) {\rm i} + bd {\rm i}^2 = ac -bd + (bc + ad) {\rm i} \\
(a + b \epsilon) (c + d \epsilon) &= ac + (bc + ad) \epsilon + bd \epsilon^2 = ac  + (bc + ad) \epsilon 
\end{align*}
$$


### Matrix representations

Complex numbers can be thought of as $2 \times 2$ matrices, that is, $a + b {\rm i}$ is equivalent to
$$
a \underbrace_{\rm representation of 1}{I} + b \underbrace_{\rm representation of n$}{\begin{pmatrix}0 &1 \\-1 & 0\end{pmatrix}} = \begin{pmatrix}a & b \\ -b & a \end{pmatrix}
$$
Similarly, for dual numbers we have
$$
a \underbrace_{\rm representation of 1}{I} + b \underbrace_{\rm representation of \epsilon}{\begin{pmatrix}0 &1 \\0 & 0\end{pmatrix}} = \begin{pmatrix}a & b \\ 0 & a \end{pmatrix}
$$
That is, the representation of $\epsilon$ is a simple 2 × 2 nilpotent matrix.

### Connection with differentiation

Applying a polynomial to a dual number $a + b \epsilon$ tells us the derivative at $a$:

**Lemma** Suppose $p$ is a polynomial. Then
$$
p(a + b \epsilon) = p(a) + b p'(a) \epsilon
$$

**Proof**

It suffices to consider $p(x) = x^n$ for $n \geq 1$ as other polynomials follow from linearity. We proceed by induction:
The case $n = 1$ is trivial. For $n > 1$ we have 
$$
(a + b \epsilon)^n = (a + b \epsilon) (a + b \epsilon)^{n-1} = (a + b \epsilon) (a^{n-1} + (n-1) b a^{n-2} \epsilon) = a^n + b n a^{n-1} \epsilon.
$$

∎

We can extend this to other differential functions by definition, that is, if $f$ is differentiable at $a$ we define
$$
f(a + b \epsilon) := f(a) + b f'(a) \epsilon
$$
Note this definition immediately implies the product and chain rule:

**Lemma (product and chain rule)**
Suppose $f$ is differentiable at $g(a)$ and $g$
is differentiable at $a$, and extended to ${\mathbb D}$ as above. Then $q(x) := f(g(x))$ and 
$r(x) := f(x) g(x)$ satisfy:
$$
\begin{align*}
h(a+b \epsilon) &= h(a) + b h'(b) \epsion \\
r(a+b \epsilon) &= r(a) + b r'(b) \epsion
\end{align*}
$$

**Proof**
For $h$ it follows immediately:
$$
h(a + b \epsilon) = f(g(a + b \epsilon)) = f(g(a) + b g'(a) \epsilon) = f(g(a)) + b g'(a) f'(g(a))\epsilon = h(a) + b h'(a) \epsilon
$$
For $r$ we have
$$
r(a + b \epsilon) = f(a+b \epsilon )g(a+b \epsilon )= (f(a) + b f'(a) \epsilon)(g(a) + b g'(a) \epsilon) = f(a)g(a) + b (f'(a)g(a) + f(a)g'(a)) \epsilon = r(a) +b r'(a) \epsilon
$$

∎

A simple corollary is that any function defined in terms of addition, multiplication, composition, etc.
of functions that are extended to dual numbers will be differentiable via dual numbers.

### Implementation in Julia

We now consider a simple implementation of dual numbers


What makes dual numbers so effective is that, unlike finite differences, they are not
prone to disasterous growth due to round-off errors. 
